{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import ast\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanations = {}\n",
    "\n",
    "with open(\"explanations.csv\", mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        if row:  # Check if row is not empty\n",
    "            index = row[0]\n",
    "            tensor_str = row[1]\n",
    "            tensor_list = ast.literal_eval(tensor_str)\n",
    "            explanations[index] = tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_frequencies(data, explanations):\n",
    "    node_frequencies_big = defaultdict(int)\n",
    "    node_frequencies_mid = defaultdict(int)\n",
    "    node_frequencies_small = defaultdict(int)\n",
    "\n",
    "    for i, edge_weight in explanations.items():\n",
    "        edge_weight = torch.tensor(edge_weight)\n",
    "        edge_index = data.edge_index\n",
    "\n",
    "        significant_edge_mask_big = edge_weight > 0.5\n",
    "        significant_edge_mask_mid = edge_weight > 0.1\n",
    "        significant_edge_mask_small = edge_weight > 0.01\n",
    "        significant_edge_index_big = edge_index[:, significant_edge_mask_big]\n",
    "        significant_edge_index_mid = edge_index[:, significant_edge_mask_mid]\n",
    "        significant_edge_index_small = edge_index[:, significant_edge_mask_small]\n",
    "\n",
    "        nodes_big = np.unique(significant_edge_index_big.numpy())\n",
    "        nodes_mid = np.unique(significant_edge_index_mid.numpy())\n",
    "        nodes_small = np.unique(significant_edge_index_small.numpy())\n",
    "\n",
    "        for node in nodes_big:\n",
    "            node_frequencies_big[node] += 1\n",
    "\n",
    "        for node in nodes_mid:\n",
    "            node_frequencies_mid[node] += 1\n",
    "\n",
    "        for node in nodes_small:\n",
    "            node_frequencies_small[node] += 1\n",
    "\n",
    "    return node_frequencies_big, node_frequencies_mid, node_frequencies_small\n",
    "\n",
    "\n",
    "node_freq_big, node_freq_mid, node_freq_small = get_node_frequencies(data, explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(freq, n):\n",
    "    # Step 1: Sort data by value\n",
    "    lst = sorted(freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \"\"\"Divide the list lst into n equally-sized chunks.\"\"\"\n",
    "    k, m = divmod(len(lst), n)\n",
    "    return [lst[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "\n",
    "num_bins = 10\n",
    "bins_big = divide_into_chunks(node_freq_big, num_bins)\n",
    "bins_mid = divide_into_chunks(node_freq_mid, num_bins)\n",
    "bins_small = divide_into_chunks(node_freq_small, num_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(1433, 8, heads=8)\n",
      "  (conv2): GATConv(64, 7, heads=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GAT(hidden_channels=8, heads=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      out = F.softmax(out, dim=1)\n",
    "      pred = out.argmax(dim=1)\n",
    "      correct = pred[mask] == data.y[mask]\n",
    "      acc = int(correct.sum()) / int(mask.sum())\n",
    "      return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9438, Val: 0.3780, Test: 0.4090\n",
      "Epoch: 002, Loss: 1.9364, Val: 0.5680, Test: 0.5890\n",
      "Epoch: 003, Loss: 1.9266, Val: 0.6180, Test: 0.6180\n",
      "Epoch: 004, Loss: 1.9165, Val: 0.6200, Test: 0.6210\n",
      "Epoch: 005, Loss: 1.9050, Val: 0.6620, Test: 0.6550\n",
      "Epoch: 006, Loss: 1.8944, Val: 0.7160, Test: 0.7260\n",
      "Epoch: 007, Loss: 1.8821, Val: 0.7680, Test: 0.7490\n",
      "Epoch: 008, Loss: 1.8728, Val: 0.7700, Test: 0.7710\n",
      "Epoch: 009, Loss: 1.8592, Val: 0.7940, Test: 0.7860\n",
      "Epoch: 010, Loss: 1.8463, Val: 0.7960, Test: 0.7880\n",
      "Epoch: 011, Loss: 1.8381, Val: 0.7980, Test: 0.7830\n",
      "Epoch: 012, Loss: 1.8246, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 013, Loss: 1.8105, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 014, Loss: 1.7909, Val: 0.7920, Test: 0.7770\n",
      "Epoch: 015, Loss: 1.7859, Val: 0.7880, Test: 0.7760\n",
      "Epoch: 016, Loss: 1.7616, Val: 0.7860, Test: 0.7760\n",
      "Epoch: 017, Loss: 1.7493, Val: 0.7880, Test: 0.7780\n",
      "Epoch: 018, Loss: 1.7317, Val: 0.7900, Test: 0.7760\n",
      "Epoch: 019, Loss: 1.7235, Val: 0.7900, Test: 0.7770\n",
      "Epoch: 020, Loss: 1.6997, Val: 0.7920, Test: 0.7790\n",
      "Epoch: 021, Loss: 1.6865, Val: 0.7920, Test: 0.7810\n",
      "Epoch: 022, Loss: 1.6548, Val: 0.7940, Test: 0.7780\n",
      "Epoch: 023, Loss: 1.6312, Val: 0.7940, Test: 0.7760\n",
      "Epoch: 024, Loss: 1.6387, Val: 0.7960, Test: 0.7790\n",
      "Epoch: 025, Loss: 1.6083, Val: 0.8000, Test: 0.7820\n",
      "Epoch: 026, Loss: 1.5871, Val: 0.8100, Test: 0.7860\n",
      "Epoch: 027, Loss: 1.5532, Val: 0.8100, Test: 0.7860\n",
      "Epoch: 028, Loss: 1.5537, Val: 0.8080, Test: 0.7860\n",
      "Epoch: 029, Loss: 1.5210, Val: 0.8060, Test: 0.7850\n",
      "Epoch: 030, Loss: 1.5193, Val: 0.8060, Test: 0.7840\n",
      "Epoch: 031, Loss: 1.4707, Val: 0.8040, Test: 0.7830\n",
      "Epoch: 032, Loss: 1.4364, Val: 0.8020, Test: 0.7820\n",
      "Epoch: 033, Loss: 1.4348, Val: 0.7980, Test: 0.7850\n",
      "Epoch: 034, Loss: 1.4013, Val: 0.8000, Test: 0.7850\n",
      "Epoch: 035, Loss: 1.3913, Val: 0.8020, Test: 0.7840\n",
      "Epoch: 036, Loss: 1.3776, Val: 0.8000, Test: 0.7850\n",
      "Epoch: 037, Loss: 1.3628, Val: 0.8000, Test: 0.7860\n",
      "Epoch: 038, Loss: 1.2888, Val: 0.8000, Test: 0.7860\n",
      "Epoch: 039, Loss: 1.2879, Val: 0.8000, Test: 0.7890\n",
      "Epoch: 040, Loss: 1.2481, Val: 0.8040, Test: 0.7870\n",
      "Epoch: 041, Loss: 1.2413, Val: 0.8020, Test: 0.7870\n",
      "Epoch: 042, Loss: 1.2108, Val: 0.8020, Test: 0.7900\n",
      "Epoch: 043, Loss: 1.1759, Val: 0.8020, Test: 0.7930\n",
      "Epoch: 044, Loss: 1.1855, Val: 0.8000, Test: 0.7920\n",
      "Epoch: 045, Loss: 1.1474, Val: 0.7980, Test: 0.7920\n",
      "Epoch: 046, Loss: 1.0874, Val: 0.7960, Test: 0.7910\n",
      "Epoch: 047, Loss: 1.0832, Val: 0.8040, Test: 0.7870\n",
      "Epoch: 048, Loss: 1.0557, Val: 0.8000, Test: 0.7890\n",
      "Epoch: 049, Loss: 1.0468, Val: 0.8000, Test: 0.7910\n",
      "Epoch: 050, Loss: 1.0210, Val: 0.8000, Test: 0.7930\n",
      "Epoch: 051, Loss: 1.0445, Val: 0.8000, Test: 0.7940\n",
      "Epoch: 052, Loss: 0.9531, Val: 0.8020, Test: 0.8010\n",
      "Epoch: 053, Loss: 0.9351, Val: 0.7960, Test: 0.8070\n",
      "Epoch: 054, Loss: 0.9705, Val: 0.8020, Test: 0.8100\n",
      "Epoch: 055, Loss: 0.9177, Val: 0.8040, Test: 0.8100\n",
      "Epoch: 056, Loss: 0.8657, Val: 0.8020, Test: 0.8120\n",
      "Epoch: 057, Loss: 0.8828, Val: 0.8020, Test: 0.8130\n",
      "Epoch: 058, Loss: 0.8666, Val: 0.8040, Test: 0.8130\n",
      "Epoch: 059, Loss: 0.8231, Val: 0.8020, Test: 0.8150\n",
      "Epoch: 060, Loss: 0.7970, Val: 0.8020, Test: 0.8130\n",
      "Epoch: 061, Loss: 0.8127, Val: 0.8020, Test: 0.8140\n",
      "Epoch: 062, Loss: 0.7456, Val: 0.7960, Test: 0.8100\n",
      "Epoch: 063, Loss: 0.7709, Val: 0.7940, Test: 0.8100\n",
      "Epoch: 064, Loss: 0.7811, Val: 0.7920, Test: 0.8070\n",
      "Epoch: 065, Loss: 0.6897, Val: 0.7860, Test: 0.8060\n",
      "Epoch: 066, Loss: 0.7240, Val: 0.7860, Test: 0.8020\n",
      "Epoch: 067, Loss: 0.6921, Val: 0.7820, Test: 0.8020\n",
      "Epoch: 068, Loss: 0.7099, Val: 0.7820, Test: 0.8020\n",
      "Epoch: 069, Loss: 0.6707, Val: 0.7820, Test: 0.8030\n",
      "Epoch: 070, Loss: 0.6232, Val: 0.7800, Test: 0.7990\n",
      "Epoch: 071, Loss: 0.6546, Val: 0.7800, Test: 0.7980\n",
      "Epoch: 072, Loss: 0.5976, Val: 0.7820, Test: 0.7980\n",
      "Epoch: 073, Loss: 0.5988, Val: 0.7820, Test: 0.8000\n",
      "Epoch: 074, Loss: 0.6181, Val: 0.7800, Test: 0.8020\n",
      "Epoch: 075, Loss: 0.5803, Val: 0.7860, Test: 0.8050\n",
      "Epoch: 076, Loss: 0.5371, Val: 0.7860, Test: 0.8080\n",
      "Epoch: 077, Loss: 0.5826, Val: 0.7900, Test: 0.8110\n",
      "Epoch: 078, Loss: 0.5908, Val: 0.7860, Test: 0.8150\n",
      "Epoch: 079, Loss: 0.5684, Val: 0.7840, Test: 0.8150\n",
      "Epoch: 080, Loss: 0.5586, Val: 0.7860, Test: 0.8170\n",
      "Epoch: 081, Loss: 0.5833, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 082, Loss: 0.5246, Val: 0.7880, Test: 0.8180\n",
      "Epoch: 083, Loss: 0.5375, Val: 0.7880, Test: 0.8160\n",
      "Epoch: 084, Loss: 0.5288, Val: 0.7860, Test: 0.8130\n",
      "Epoch: 085, Loss: 0.5622, Val: 0.7840, Test: 0.8060\n",
      "Epoch: 086, Loss: 0.4883, Val: 0.7800, Test: 0.8050\n",
      "Epoch: 087, Loss: 0.5044, Val: 0.7800, Test: 0.8040\n",
      "Epoch: 088, Loss: 0.5221, Val: 0.7820, Test: 0.8010\n",
      "Epoch: 089, Loss: 0.5142, Val: 0.7780, Test: 0.7990\n",
      "Epoch: 090, Loss: 0.5283, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 091, Loss: 0.5168, Val: 0.7800, Test: 0.8020\n",
      "Epoch: 092, Loss: 0.4432, Val: 0.7800, Test: 0.8040\n",
      "Epoch: 093, Loss: 0.4656, Val: 0.7780, Test: 0.8010\n",
      "Epoch: 094, Loss: 0.4310, Val: 0.7760, Test: 0.8000\n",
      "Epoch: 095, Loss: 0.4199, Val: 0.7820, Test: 0.8060\n",
      "Epoch: 096, Loss: 0.4257, Val: 0.7800, Test: 0.8080\n",
      "Epoch: 097, Loss: 0.4855, Val: 0.7820, Test: 0.8080\n",
      "Epoch: 098, Loss: 0.4159, Val: 0.7800, Test: 0.8090\n",
      "Epoch: 099, Loss: 0.4419, Val: 0.7800, Test: 0.8100\n",
      "Epoch: 100, Loss: 0.4532, Val: 0.7800, Test: 0.8100\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    test_acc = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(data.x.shape)\n",
    "print(min([key for key, val in node_freq_big]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "\n",
    "def generate_perturbations(data, nodes_to_remove):\n",
    "    data = copy.deepcopy(data)\n",
    "    nodes_to_remove = set(nodes_to_remove)  # Convert tensor to set of node indices\n",
    "    print(len(nodes_to_remove))\n",
    "\n",
    "    # # Get the current edge list and convert it to a list of tuples\n",
    "    # edges = data.edge_index.t()\n",
    "    # edges_list = edges.tolist()\n",
    "    \n",
    "    # # Filter out edges that connect to the nodes to be removed\n",
    "    # filtered_edges = [edge for edge in edges_list if edge[0] not in nodes_to_remove and edge[1] not in nodes_to_remove]\n",
    "    \n",
    "    # # Update the edge indices in the data\n",
    "    # data.edge_index = torch.tensor(filtered_edges).t()\n",
    "    \n",
    "    # Create a mask that is False for nodes to remove and True for others\n",
    "    mask = torch.ones(data.x.size(0), dtype=torch.bool)  # data.x.size(0) is the number of nodes\n",
    "    mask[list(nodes_to_remove)] = False\n",
    "    \n",
    "    data.x = data.x[mask]\n",
    "\n",
    "    all_nodes = set(range(2708))\n",
    "    print(len(all_nodes))\n",
    "    nodes_to_retain = all_nodes.difference(nodes_to_remove)\n",
    "    print(len(nodes_to_retain))\n",
    "    print(data.edge_index.shape)\n",
    "    data.edge_index = pyg_utils.subgraph(sorted(nodes_to_retain), data.edge_index, relabel_nodes=True)[0]\n",
    "    print(data.edge_index.shape)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_logit_diff(data, perturbed_data, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_orig = model(data.x, data.edge_index)\n",
    "        out_perturb = model(perturbed_data.x, perturbed_data.edge_index)\n",
    "        \n",
    "        # Normalize outputs by their corresponding inputs\n",
    "        norm_out_orig = torch.norm(out_orig, p=2) / torch.norm(data.x, p=2)\n",
    "        norm_out_perturb = torch.norm(out_perturb, p=2) / torch.norm(perturbed_data.x, p=2)\n",
    "        \n",
    "        # Compute the logit difference\n",
    "        l2_norm = norm_out_orig - norm_out_perturb\n",
    "        \n",
    "    return l2_norm\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8748])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8956])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8842])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8750])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8810])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8860])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8698])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8686])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8338])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8740])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9070])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9078])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9048])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 8988])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9096])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9132])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9216])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9086])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9124])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9084])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9204])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9262])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9272])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9308])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9254])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9302])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9278])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9306])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9276])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9194])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9400])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9394])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9334])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9348])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9352])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9372])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9332])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9324])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9370])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9410])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9486])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9480])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9512])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9506])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9536])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9482])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9422])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9478])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9520])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9516])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9642])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9656])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9664])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9660])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9632])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9664])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9640])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9662])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9626])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9610])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9792])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9792])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9806])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9762])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9760])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9822])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9798])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9770])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9798])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9796])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9950])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9910])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9930])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9946])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9934])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9920])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9932])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9936])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9926])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 9936])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10132])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10130])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10136])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10148])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10148])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10146])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10134])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10154])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10156])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10156])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10286])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10294])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10294])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10286])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10292])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10290])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10288])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10290])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10284])\n",
      "135\n",
      "2708\n",
      "2573\n",
      "torch.Size([2, 10556])\n",
      "torch.Size([2, 10292])\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "results = torch.zeros(num_bins, num_samples)\n",
    "\n",
    "for i, bin in enumerate(bins_small):\n",
    "    for j in range(num_samples):\n",
    "        half_size = len(bin) // 2\n",
    "        sampled_nodes = random.sample(bin, half_size)\n",
    "        nodes_to_remove = [i[0] for i in sampled_nodes]\n",
    "        perturbed_data = generate_perturbations(data, nodes_to_remove)\n",
    "        results[i, j] = get_logit_diff(data, perturbed_data, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
